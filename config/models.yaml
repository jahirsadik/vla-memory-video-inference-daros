# models:
#   - name: "Qwen3-VL-8B"
#     model_path: "Qwen/Qwen3-VL-8B-Instruct"
#     port: 30000
#     host: "localhost"
#     attention_backend: "flashinfer"
#     mm_attention_backend: "flashinfer"
#     enabled: true

  # Add more models below
  # - name: "Llava-1.6"
  #   model_path: "llavav1.6"
  #   port: 30001
  #   host: "localhost"
  #   attention_backend: "flashinfer"
  #   mm_attention_backend: "flashinfer"
  #   enabled: false

models:

  - name: "Qwen3-VL-2B"
    model_path: "Qwen/Qwen3-VL-2B-Instruct"
    port: 30000
    host: "localhost"
    enabled: false

  - name: "Qwen3-VL-4B"
    model_path: "Qwen/Qwen3-VL-4B-Instruct"
    port: 30001
    host: "localhost"
    enabled: false

  - name: "Qwen3-VL-8B"
    model_path: "Qwen/Qwen3-VL-8B-Instruct"
    port: 30002
    host: "localhost"
    enabled: false

  - name: "Qwen3-VL-32B"
    model_path: "Qwen/Qwen3-VL-32B-Instruct"
    port: 30003
    host: "localhost"
    enabled: false

  - name: "Qwen3-VL-2B-Thinking"
    model_path: "Qwen/Qwen3-VL-2B-Thinking"
    port: 30004
    host: "localhost"
    enabled: false

  - name: "Qwen3-VL-4B-Thinking"
    model_path: "Qwen/Qwen3-VL-4B-Thinking"
    port: 30005
    host: "localhost"
    enabled: false

  - name: "Qwen3-VL-8B-Thinking"
    model_path: "Qwen/Qwen3-VL-8B-Thinking"
    port: 30006
    host: "localhost"
    enabled: false

  - name: "InternVL3_5-14B"
    model_path: "OpenGVLab/InternVL3_5-14B"
    port: 30007
    host: "localhost"
    enabled: false

  - name: "InternVL3_5-8B"
    model_path: "OpenGVLab/InternVL3_5-8B"
    port: 30008
    host: "localhost"
    enabled: false

  - name: "InternVL3_5-4B"
    model_path: "OpenGVLab/InternVL3_5-4B"
    port: 30009
    host: "localhost"
    enabled: false

  - name: "InternVL3_5-2B"
    model_path: "OpenGVLab/InternVL3_5-2B"
    port: 30010
    host: "localhost"
    enabled: false

  - name: "InternVL3_5-1B"
    model_path: "OpenGVLab/InternVL3_5-1B"
    port: 30011
    host: "localhost"
    enabled: false

  - name: "InternVL3_5-GPT-OSS-20B-A4B-Preview"
    model_path: "OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview"
    port: 30012
    host: "localhost"
    enabled: false

  - name: "LLaVA-NeXT-Video-7B"
    model_path: "lmms-lab/LLaVA-NeXT-Video-7B"
    port: 30020
    host: "localhost"
    enabled: true

  - name: "InternVL2.5-26B"
    model_path: "OpenGVLab/InternVL2_5-26B"
    port: 30021
    host: "localhost"
    enabled: false

  - name: "LLaVA-Video-7B"
    model_path: "lmms-lab/LLaVA-Video-7B-Qwen2"
    port: 30022
    host: "localhost"
    enabled: false

  - name: "Llama-3.2-Vision"
    model_path: "meta-llama/Llama-3.2-11B-Vision-Instruct"
    port: 30023
    host: "localhost"
    enabled: false  # Gated model - needs HF token

  - name: "Pixtral-12B"
    model_path: "mistralai/Pixtral-12B-2409"
    port: 30024
    host: "localhost"
    enabled: false

  - name: "Molmo-7B"
    model_path: "allenai/Molmo-7B-D-0924"
    port: 30025
    host: "localhost"
    enabled: false

  - name: "MiniCPM-V-2.6"
    model_path: "openbmb/MiniCPM-V-2_6"
    port: 30026
    host: "localhost"
    enabled: false

inference:
  max_tokens: 2048
  temperature: 0.1  # Set to 0 for precise counting tasks
  top_k: null       # Set to integer to limit top-k sampling (e.g., 50)
  top_p: null       # Set to float for nucleus sampling (e.g., 0.9)
  timeout: 1000       # seconds

directories:
  videos: "./videos"
  results: "./results"

logging:
  level: "INFO"
  log_file: "./inference.log"
